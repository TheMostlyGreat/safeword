---
description: BDD orchestrator for feature-level work requiring multiple scenarios. Use when user says 'add', 'implement', 'build', 'feature', 'iteration', 'story', 'phase', or references an iteration/story from a spec. Also use when work touches 3+ files with new state/flows, or when user runs /bdd. Do NOT use for bug fixes, typos, config changes, or 1-2 file tasks.
alwaysApply: false
---

# BDD Orchestrator

Behavior-first development for features. Discovery → Scenarios → Implementation.

**Iron Law:** DEFINE BEHAVIOR BEFORE IMPLEMENTATION

## Phase Tracking

Features progress through phases. Track in ticket frontmatter:

```yaml
---
type: feature
phase: implement # intake | define-behavior | scenario-gate | decomposition | implement | done
---
```

**Phase meanings:**

| Phase             | What happens                         |
| ----------------- | ------------------------------------ |
| `intake`          | Context check, discovery (Phase 0-2) |
| `define-behavior` | Writing Given/When/Then (Phase 3)    |
| `scenario-gate`   | Validating scenarios (Phase 4)       |
| `decomposition`   | Task breakdown (Phase 5)             |
| `implement`       | Outside-in TDD (Phase 6)             |
| `done`            | Cleanup, verification (Phase 7)      |

**Update phase when:**

- Completing a BDD phase → set next phase
- Handing off to TDD → set `implement`
- All scenarios pass → set `done`

---

## Resume Logic

When user references a ticket, resume work:

1. **Read ticket** → get current `phase:`
2. **Find progress** → first unchecked `[ ]` in test-definitions
3. **Check context** → read last work log entry
4. **Announce resume** → "Resuming at [phase]. Last: [log entry]."

**Resume by phase:**

| Phase             | Resume action                          |
| ----------------- | -------------------------------------- |
| `intake`          | Start context check (Phase 0-2)        |
| `define-behavior` | Continue drafting scenarios            |
| `scenario-gate`   | Continue validating scenarios          |
| `decomposition`   | Continue task breakdown                |
| `implement`       | Find first unchecked scenario, run TDD |
| `done`            | Run /verify and /audit checks          |

---

## Phase 0-2: Context Check & Discovery

**Entry:** Agent detects feature-level work OR resumes ticket at `intake` phase.

### Context Check

Check if spec exists with required context:

1. **Read spec** (or note if missing)
2. **Check for goal AND scope sections**
3. **If missing or incomplete** → ask context questions:
   - "What's the goal? What should users be able to do?"
   - "What's in scope? What are we building?"
   - "What's explicitly out of scope?"
4. **Create/update spec** with answers

**Exit context check:** Spec has goal AND scope sections.

**Edge case:** User gives partial answer (goal but not scope) → Ask only for missing field, don't re-ask answered questions.

### Discovery (Optional)

After context check, offer discovery:

> "Want to spitball edge cases before we dive in?"

**If user declines** (or says "ready") → update ticket to `define-behavior`, proceed to Phase 3.

**If user accepts** → run discovery rounds:

1. Ask 2-3 PM-style questions per round (see AGENTS.md "PM-Style Questions"):
   - Round 1: User experience ("What does success feel like?")
   - Round 2: Failure modes ("What breaks? What are consequences?")
   - Round 3: Boundaries ("What's the minimum? Maximum?")
   - Round 4: Scenarios ("Walk through a concrete situation")
   - Round 5: Regret ("If we skip this, what support tickets come?")
2. Capture insights in spec under "## Discovery" section
3. After each round: "Another round or ready to proceed?"
4. **Max 5 rounds** — after round 5, proceed automatically

**Exit discovery:** User says "ready" OR max rounds reached → update ticket to `define-behavior`.

**Example round:**

> Agent: "Round 2 - Failure modes. What happens when a session expires mid-flow? What's the worst-case scenario?"
> User: "They lose progress. We'd get support tickets about lost work."
> Agent: "Got it - session expiry = data loss risk. Another round or ready?"

---

## Phase 3: Define Behavior

**Entry:** Agent enters `define-behavior` phase (after discovery or resume)

**Draft scenarios:**

1. Read spec goal/scope
2. Draft Given/When/Then scenarios covering:
   - Happy path (main success)
   - Failure modes (what can go wrong)
   - Edge cases (boundaries, empty states)
3. Present scenarios to user
4. User can add/modify/remove scenarios
5. Save to `.safeword-project/test-definitions/feature-{slug}.md`
6. Each scenario gets `[ ]` checkbox for implementation tracking

**Exit:** User approves scenario list → update ticket to `phase: scenario-gate`

---

## Phase 4: Scenario Quality Gate

**Entry:** Agent enters `scenario-gate` phase

**Validate each scenario against three criteria:**

| Criterion         | Check                          | Red flag                        |
| ----------------- | ------------------------------ | ------------------------------- |
| **Atomic**        | Tests ONE behavior             | Multiple When/Then pairs        |
| **Observable**    | Has externally visible outcome | Internal state only             |
| **Deterministic** | Same result on repeated runs   | Time/random/external dependency |

**Report issues:**

- Group by type (atomicity, observability, determinism)
- Suggest fix for each issue
- Example: "Scenario 3 tests login AND session creation. Split into two scenarios."

**Exit options:**

- All pass → update ticket to `decomposition`
- Issues found → user fixes or acknowledges → update ticket to `decomposition`

---

## Phase 5: Technical Decomposition

**Entry:** Agent enters `decomposition` phase (after scenarios validated)

**Analyze scenarios for implementation:**

1. **Identify components** — What parts of the system does each scenario touch?
   - UI components
   - API endpoints
   - Data models
   - Business logic modules
2. **Assign test layers** — For each component:
   - Pure logic (no I/O) → unit test
   - API boundaries, database → integration test
   - User flows → E2E test
3. **Create task breakdown** — Order by dependencies:
   - Data models first
   - Business logic second
   - API endpoints third
   - UI components fourth
   - E2E tests last (prove everything works)
4. **Present to user** — Show components, test layers, task order

**Complex features** (3+ components, new tech choices, schema changes) may warrant documentation first. See `.safeword/guides/design-doc-guide.md` for feature-level decisions, `.safeword/guides/architecture-guide.md` for cross-cutting choices.

**Exit:** User approves breakdown → update ticket to `phase: implement`

---

## Phase 6: Implementation (TDD)

**Entry:** Agent enters `implement` phase (after decomposition complete)

**Iron Law:** NO IMPLEMENTATION UNTIL TEST FAILS FOR THE RIGHT REASON

Announce: "Entering implementation. TDD mode for each scenario."

### Outside-In Test Layering

Build tests from the outside in:

1. **E2E first** — Prove the user-facing behavior works end-to-end
2. **Integration** — Test component boundaries with real dependencies where practical
3. **Unit** — Test isolated logic, mock external dependencies only when necessary

This ensures you're testing real behavior, not implementation details.

### Test Fixtures

For complex test data:

- Create factory functions (e.g., `createTestUser()`) instead of inline objects
- Keep fixtures close to tests that use them
- Name fixtures by scenario, not by data shape (e.g., `userWithExpiredSubscription`)

### Walking Skeleton (first scenario only)

If project has no E2E infrastructure, build skeleton first:

- Thinnest possible slice proving architecture works
- Form → API → response → UI (no real logic)
- This becomes foundation for all scenarios

### For Each Scenario (RED → GREEN → REFACTOR):

#### 6.1 RED - Write Failing Test

1. Pick ONE test from test-definitions (first unchecked `[ ]`)
2. Write E2E test code (from Given/When/Then)
3. Run test → verify fails for RIGHT reason (behavior missing, not syntax)
4. Commit: `test: [scenario name]`

**Red Flags → STOP:**

| Flag                    | Action                           |
| ----------------------- | -------------------------------- |
| Test passes immediately | Rewrite - you're testing nothing |
| Syntax error            | Fix syntax, not behavior         |
| Wrote implementation    | Delete it, return to test        |
| Multiple tests at once  | Pick ONE                         |

#### 6.2 GREEN - Minimal Implementation

**Iron Law:** ONLY WRITE CODE THE TEST REQUIRES

1. Write minimal code to pass test
2. Run test → verify passes
3. Run FULL test suite → verify no regressions
4. Commit: `feat: [scenario name]`

**Verification Gate:** Evidence before claims.

| Claim            | Requires                      | Not Sufficient              |
| ---------------- | ----------------------------- | --------------------------- |
| "Tests pass"     | Fresh test output: 0 failures | "should pass", previous run |
| "Build succeeds" | Build command: exit 0         | "linter passed"             |

**Anti-Pattern: Mock Implementations**

Don't hardcode values to pass tests:

```typescript
// ❌ BAD - Hardcoded to pass test
function calculate(x) {
  return 42;
}

// ✅ GOOD - Actual logic
function calculate(x) {
  return x * 2;
}
```

#### 6.3 REFACTOR - Clean Up

**Iron Law:** TESTS MUST PASS BEFORE AND AFTER EVERY CHANGE

| Smell                     | Action                           |
| ------------------------- | -------------------------------- |
| Duplication introduced    | Extract shared function/constant |
| Unclear name              | Rename to reveal intent          |
| Long function (>20 lines) | Extract helper                   |
| Magic number/string       | Extract constant                 |
| No obvious improvements   | Skip refactor, proceed to 6.4    |

**Protocol:**

1. Run tests → must pass (baseline)
2. Make ONE refactoring change
3. Run tests → must still pass
4. Commit: `refactor: [what improved]`
5. Repeat if more smells exist

**Revert if tests fail:**

```bash
git checkout -- <changed-files>
```

Then try a smaller change or skip refactoring.

#### 6.4 Mark & Iterate

1. Mark scenario `[x]` in test-definitions
2. Return to 6.1 for next scenario, or proceed to Phase 7 if all done

---

## Phase 7: Done Gate

**Entry:** All scenarios marked `[x]` in test-definitions

**Before marking ticket `status: done`, complete this checklist:**

### Required (run /done to automate)

- [ ] All scenarios `[x]` in test-definitions
- [ ] Full test suite passes
- [ ] Build succeeds
- [ ] Lint passes
- [ ] Run `/audit` — no errors (warnings OK)

### Flake Detection

Run tests multiple times (3x recommended) to catch flaky tests:

```bash
for i in {1..3}; do bun test || echo "FLAKE DETECTED on run $i"; done
```

If any run fails inconsistently, investigate before marking done.

### Cross-Scenario Refactoring

After all scenarios pass, look for cleanup opportunities across the feature:

| Pattern                     | Action                                    |
| --------------------------- | ----------------------------------------- |
| Duplicate setup code        | Extract shared fixture or helper          |
| Similar test assertions     | Create custom matcher or assertion helper |
| Repeated mock configuration | Create mock factory                       |
| Copy-pasted business logic  | Extract shared module                     |

This is OPTIONAL — only refactor if clear wins exist. Don't gold-plate.

### Scenario Tagging (if using test runner that supports it)

Consider tagging scenarios for selective test runs:

- `@smoke` — Critical path tests, run on every commit
- `@regression` — Full test suite, run before release
- `@slow` — Long-running tests, exclude from watch mode

### Parent Epic (if applicable)

If ticket has `parent:` field:

1. Add completion entry to parent's work log
2. If all `children:` done → update parent `status: done`

### Final Commit

1. Update ticket: `phase: done`, `status: done`, `last_modified: [now]`
2. Commit: `feat(scope): [summary]`

---

## Current Behavior

1. Detect work level (see SAFEWORD.md "Work Level Detection")
2. **If user references iteration/story/phase from a spec:**
   - Check if child ticket exists for that iteration
   - If not → create ticket, run full BDD (this IS feature-level work)
   - If yes → resume at current phase
3. Announce with override hint
4. **If ticket exists:** Read phase, resume at appropriate point
5. **Artifact-first rule:** Before doing work, create/verify the phase artifact:
   - Phase 0-2 → ticket at `.safeword-project/issues/XXX-*.md`
   - Phase 3 → test-definitions at `.safeword-project/test-definitions/XXX-*.md`
   - Phase 5 → task breakdown in ticket
6. **Phase 0-2:** Context check (goal/scope), optional discovery
7. **Phase 3:** Draft scenarios from spec, save to test-definitions
8. **Phase 4:** Validate scenarios (atomic, observable, deterministic)
9. **Phase 5:** Decompose into components, assign test layers, create task breakdown
10. **Phase 6:** TDD implementation (RED → GREEN → REFACTOR for each scenario)
11. **Phase 7:** Done gate checklist, parent epic update, final commit
12. **Update phase** in ticket when transitioning

---

## Decomposition Checkpoints

Check for splitting at these points. Splitting is **suggested, not mandatory**—user decides.

### When to Split

| Checkpoint  | Trigger                                 | Action                     |
| ----------- | --------------------------------------- | -------------------------- |
| **Entry**   | Multiple distinct outcomes, vague scope | Split into epic + features |
| **Phase 3** | >15 scenarios OR 3+ distinct clusters   | Split by user journey      |
| **Phase 5** | >20 tasks OR 5+ major components        | Split by component/layer   |

**Epic detection heuristic:** Can't describe in ONE "As a [user] I want [thing] so that [value]" → probably an epic.

### Split Protocol

When user accepts a split:

1. **Create parent ticket** (epic) with `children:` array
2. **Create child tickets** with:
   - `parent:` linking to epic
   - `phase:` set to appropriate restart point (see below)
3. **Commit:** "chore: split [name] into N features"
4. **Ask:** "Which feature should we start with?"

**Restart points after split:**

| Split At | Child Restarts From | Why                                     |
| -------- | ------------------- | --------------------------------------- |
| Entry    | `intake`            | New features need their own discovery   |
| Phase 3  | `scenario-gate`     | Scenarios exist, validate regrouped set |
| Phase 5  | `implement`         | Decomposition done, start implementing  |

### Example: Entry Split

```
Epic: 001-user-auth
├── type: epic
├── children: [002, 003]
│
├── 002-login-flow
│   ├── parent: 001
│   └── phase: intake
│
└── 003-registration-flow
    ├── parent: 001
    └── phase: intake
```

Parent completes when ALL children reach `done`.

---

## Key Takeaways

- **patch/task** → TDD directly (RED → GREEN → REFACTOR)
- **feature** → full BDD flow (Phases 0-7), track in ticket `phase:` field
- **Resume** → read ticket, find first unchecked scenario, continue
- **Split** → check thresholds at Entry, Phase 3, Phase 5; user decides
- **Done gate** → run /done before marking ticket complete
- When unsure → default to task, user can `/bdd` to override
